# pyright: reportMissingImports=false
# pylint: disable=import-error,no-name-in-module
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score
from imblearn.over_sampling import SMOTE
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.regularizers import l2
from tensorflow.keras.callbacks import EarlyStopping
import json
from joblib import dump

# Load dataset
try:
   heart_data = pd.read_csv(r'C:\Users\himan\OneDrive\Desktop\cardialink-quantify\src\components\model\heart_disease_data.csv')

except FileNotFoundError:
    print("Error: Dataset file not found. Please check the file path.")
    exit()

print(heart_data.head())

# Define features and target
# 'target' is assumed to be the column for heart disease status
X = heart_data.drop('target', axis=1)
Y = heart_data['target']

# Split data into training and testing sets
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, random_state=2)

# Standardize features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to balance the dataset (if needed)
print("Class distribution before SMOTE:", np.unique(Y_train, return_counts=True))
smote = SMOTE(random_state=42)
X_train_res, Y_train_res = smote.fit_resample(X_train_scaled, Y_train)
print("Class distribution after SMOTE:", np.unique(Y_train_res, return_counts=True))

# Build the Neural Network Model
model = Sequential([
    Dense(64, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train_res.shape[1],)),
    Dropout(0.5),
    Dense(32, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
             loss='binary_crossentropy',
             metrics=['accuracy',
                      tf.keras.metrics.Precision(name='precision'),
                      tf.keras.metrics.Recall(name='recall')])

# Train the model
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train_res, Y_train_res, epochs=100, batch_size=32,
                   validation_split=0.2, callbacks=[early_stop], verbose=1)

# Evaluate the model
test_results = model.evaluate(X_test_scaled, Y_test)
print(f"Test Accuracy: {test_results[1]*100:.2f}%")
print(f"Test Precision: {test_results[2]*100:.2f}%")
print(f"Test Recall: {test_results[3]*100:.2f}%")

# Generate predictions
nn_test_prob = model.predict(X_test_scaled)
nn_test_pred = (nn_test_prob > 0.5).astype(int)
print(classification_report(Y_test, nn_test_pred))

# Save the trained model
model.save('heart_disease_model.h5')

# Save the scaler and feature names
dump(scaler, 'scaler.joblib')
FEATURE_NAMES = list(X.columns)
with open('feature_names.json', 'w') as f:
    json.dump(FEATURE_NAMES, f)

print("Model and scaler saved successfully.")
